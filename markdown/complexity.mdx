# Time Complexity

By definition, Time complexity is the time taken by an algorithm to run as a function of the length of the input.

<img src="/images/complexity/20240503222320.png" alt="bubble-sort-first-pass" />

| Value                  | Complexity Name             |
| ---------------------- | --------------------------- |
| $$\text{O}(1)$$        | Constant Time Complexity    |
| $$\text{O}(\log{n})$$  | Logarithmic Time Complexity |
| $$\text{O}(n)$$        | Linear Time Complexity      |
| $$\text{O}(n\log{n})$$ | Log-Linear Time Complexity  |
| $$\text{O}({n}^2)$$    | Quadratic Time Complexity   |
| $$\text{O}(2^{n})$$    | Exponential Time Complexity |
| $$\text{O}({n}!)$$     | Factorial Time Complexity   |

## The High-Level Idea

Asymptotic notations are mathematical notations which are used to describe the running time of an algorithm when input tends towards infinity.

- The goal is to identify a sweet spot of granularity for reasoning about algorithms

<img src="/images/complexity/20240503222154.png" alt="bubble-sort-first-pass" />

We want to suppress second-order details like constant factors and lower-order terms, and focus on how the running time of an algorithm scales as the input size grows.

- Lower-order terms, by definition, become increasingly irrelevant as we focus on large inputs, which are the inputs that require algorithmic ingenuity.
- The constant factors are generally highly dependent on the details of the environment.

If we don‚Äôt want to commit to a speciÔ¨Åc programming language, architecture, or compiler when analysing an algorithm, it makes sense to use a formalism that does not focus on constant factors.
When comparing algorithms asymptotic analysis is the right tool for understanding which one is going to perform better, especially on reasonably large inputs.

## Big-O Notation

- Intuitively, something is $$\text{O(f}(n))$$ for a function $$\text{f}(n)$$ means that $$\text{f}(n)$$ is what you‚Äôre left with after suppressing constant factors and lower-order terms.
- Big-O is analogous to ‚Äúless than or equal to ( $$\leq$$ ),‚Äù
- Big-O notations give us a convenient way to talk about upper bounds.
- The Big-O notation describes the worst-case running time of an algorithm. It is computed by counting the number of operations it will take in the worst-case scenario with the input $$n$$.

## Big-Omega

- Big-omega ($$\Omega$$) is analogous to ‚Äúgreater than or equal to ( $$\geq$$ )‚Äù.

- The notation describes the best running time of an algorithm. It is computed by counting the number of operations it will take in the best-case scenario with the input $$n$$.

## Big-Theta Notation

- Big-theta is ($$\Theta$$) analogous to ‚Äúequal to (=),‚Äù
- The theta notation encloses the function from above and below, therefore it defines the exact asymptotic behaviour. The notation is used for analysing the average runtime of an algorithm.

> üí° Algorithm designers often use big-O notation even when big-theta notation would be more accurate. This is because, as algorithm designers, we generally focus on upper bounds‚Äîguarantees about how long our algorithms could possibly run.

## Little-O Notation

- Big-O notation is analogous to ‚Äúless than or equal to,‚Äù little-o notation is analogous to ‚Äústrictly less than.‚Äù
- Similarly, there is a ‚Äúlittle-omega‚Äù notation that corresponds to ‚Äústrictly greater than‚Äù
