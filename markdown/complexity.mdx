# Time Complexity

By definition, Time complexity is the time taken by an algorithm/program to run as a function of the length of the input.

| Value                  | Complexity Name             |
| ---------------------- | --------------------------- |
| $$\text{O}(1)$$        | Constant Time Complexity    |
| $$\text{O}(\log{n})$$  | Logarithmic Time Complexity |
| $$\text{O}(n)$$        | Linear Time Complexity      |
| $$\text{O}(n\log{n})$$ | Log-Linear Time Complexity  |
| $$\text{O}({n}^2)$$    | Quadratic Time Complexity   |
| $$\text{O}(2^{n})$$    | Exponential Time Complexity |
| $$\text{O}({n}!)$$     | Factorial Time Complexity   |

- Asymptotic notations are mathematical notations which are used to describe the running time of an algorithm when input tends towards infinity.
  - The goal is to identify a sweet spot of granularity for reasoning about algorithms
  - We want to suppress second-order details like constant factors and lower-order terms, and focus on how the running time of an algorithm scales as the input size grows.

# The High-Level Idea

- Lower-order terms, by definition, become increasingly irrelevant as we focus on large inputs, which are the inputs that require algorithmic ingenuity.

- The constant factors are generally highly dependent on the details of the environment.

  - If we don’t want to commit to a speciﬁc programming language, architecture, or compiler when analysing an algorithm, it makes sense to use a formalism that does not focus on constant factors.

- When comparing algorithms asymptotic analysis is the right tool for understanding which one is going to perform better, especially on reasonably large inputs.

# Big-O Notation

- Intuitively, something is $$\text{O(f}(n))$$ for a function $$\text{f}(n)$$ means that $$\text{f}(n)$$ is what you’re left with after suppressing constant factors and lower-order terms.

Big-O Notation (Mathematical Version)

$$\text{T}(n) = \text{O(f}(n))$$ if and only if $$\text{T}(n)$$ is eventually bounded above by a constant multiple of $$\text{f}(n)$$ i.e there exist positive constants $$c$$ and $$n_0$$ such that

$$ \text{T}(n) \leq c\*\text{f}(n) $$ for all $$n \geq n_0$$

### Example

- The x-axis corresponds to the parameter n
- The y-axis to the value of a function.
- $$\text{T}(n)$$ be the function corresponding to the solid line, and $$\text{f}(n)$$ the lower dashed line.
- $$\text{T}(n)$$ is not bounded above by $$\text{f}(n)$$ but multiplying f(n) by 3 results in the upper dashed line, which does lie above $$\text{T}(n)$$ once we go far enough to the right on the graph, after the “crossover point” at $$n_0$$ .
- Since $$\text{T}(n)$$ is indeed eventually bounded above by a constant multiple of $$\text{f}(n)$$, we can say that $$\text{T}(n) = \text{O(f}(n))$$ </aside>

- Big-O is analogous to “less than or equal to ( $$\leq$$ ),”
- Big-O notations give us a convenient way to talk about upper bounds.
- The Big-O notation describes the worst-case running time of an algorithm. It is computed by counting the number of operations it will take in the worst-case scenario with the input $$n$$.

# Big-Omega

- Big-omega ($$\Omega$$) is analogous to “greater than or equal to ( $$\geq$$ )”.

- Function $$\text{T}(n)$$ is big-omega of another function $$\text{f}(n)$$ i.e $$\text{T}(n) = \Omega (f(n))$$ if and only if $$\text{T}(n)$$ is eventually bounded below by a constant multiple of $$\text{f}(n)$$ i.e there exist positive constants $$c$$ and $$n_0$$ such that

$$ \text{T}(n) \geq c\*\text{f}(n) $$

for all $$n \geq n_0$$

- The notation describes the best running time of an algorithm. It is computed by counting the number of operations it will take in the best-case scenario with the input $$n$$.

# Big-Theta Notation

- Big-theta is ($$\Theta$$) analogous to “equal to (=),”

- $$\text{T}(n) = \Theta(f(n))$$ if and only if there exist positive constants $$c_1$$ , $$c_2$$ , and $$n_0$$ such that

$$ c_1*\text{f}(n) \leq \text{T}(n) \leq c_2*\text{f}(n) $$

for all $$n \geq n_0$$

- It means that both $$\text{T}(n) = \Omega (\text{f}(n))$$ and $$\text{T}(n) = \text{O} (\text{f}(n))$$.

- The theta notation encloses the function from above and below, therefore it defines the exact asymptotic behaviour. The notation is used for analysing the average runtime of an algorithm.

> Algorithm designers often use big-O notation even when big-theta notation would be more accurate. This is because, as algorithm designers, we generally focus on upper bounds—guarantees about how long our algorithms could possibly run.

# Little-O Notation

- Big-O notation is analogous to “less than or equal to,” little-o notation is analogous to “strictly less than.”
- Similarly, there is a “little-omega” notation that corresponds to “strictly greater than”
